{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = ('../data/edgar_filings2.csv')\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=file_path,\n",
    "    encoding=\"utf-8\",\n",
    "    csv_args={\n",
    "        \"delimiter\": \"|\",\n",
    "        \"quotechar\": '\"'\n",
    "    },\n",
    "    metadata_columns=[\"company\", 'date', 'section']\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "texts=text_splitter.split_documents(data)\n",
    "\n",
    "test_texts=texts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/edgar_filings2.csv', 'row': 0, 'company': 'HUBS', 'date': '2024-02-14', 'section': 'business'}, page_content='text: page no. part i item 1. business 7 item 1a. risk factors 16 item 1b. unresolved staff comments 43 item 1c. cybersecurity item 2. properties item 3. legal proceedings 44 item 4. mine safety disclosures part ii item 5. market for registrant’s common equity, related stockholder matters and issuer purchases of equity securities item 6. [reserved] item 7. management’s discussion and analysis of financial condition and results of operations item 7a. quantitative and qualitative disclosures about market risk item 8. financial statements and supplementary data item 9. changes in and disagreements with accountants on accounting and financial disclosures 97 item 9a. controls and procedures item 9b. other information item 9c. disclosure regarding foreign jurisdictions that prevent inspections part iii item 10. directors, executive officers and corporate governance item 11. executive compensation 99 item 12. security ownership of certain beneficial owners and management and related'),\n",
       " Document(metadata={'source': '../data/edgar_filings2.csv', 'row': 0, 'company': 'HUBS', 'date': '2024-02-14', 'section': 'business'}, page_content='part iii item 10. directors, executive officers and corporate governance item 11. executive compensation 99 item 12. security ownership of certain beneficial owners and management and related stockholder matters 99 item 13. certain relationships and related transactions, and director independence 99 item 14. principal accounting fees and services part iv item 15. exhibits, financial statement schedules item 16. 10-k summary 100 signatures 103'),\n",
       " Document(metadata={'source': '../data/edgar_filings2.csv', 'row': 1, 'company': 'HUBS', 'date': '2024-02-14', 'section': 'business'}, page_content='text: special note regarding forward-looking statements this annual report on form 10-k contains forward-looking statements within the meaning of section 27a of the securities act of 1933, as amended, and section 21e of the securities exchange act of 1934, as amended, and these statements involve substantial risks and uncertainties. all statements other than statements of historical fact contained in this annual report on form 10-k are forward-looking statements. forward-looking statements generally relate to future events or our future financial or operating performance. in some cases, you can identify forward-looking statements because they contain words such as “may,” “should,” “expects,” “plans,” “anticipates,” “could,” “intends,” “target,” “projects,” “contemplates,” “believes,” “estimates,” “predicts,” “potential” or “continue” or the negative of these words or other similar terms or expressions that concern our expectations, strategy, plans or intentions. forward-looking'),\n",
       " Document(metadata={'source': '../data/edgar_filings2.csv', 'row': 1, 'company': 'HUBS', 'date': '2024-02-14', 'section': 'business'}, page_content='“estimates,” “predicts,” “potential” or “continue” or the negative of these words or other similar terms or expressions that concern our expectations, strategy, plans or intentions. forward-looking statements contained in this annual report on form 10-k include, but are not limited to, statements about:'),\n",
       " Document(metadata={'source': '../data/edgar_filings2.csv', 'row': 2, 'company': 'HUBS', 'date': '2024-02-14', 'section': 'business'}, page_content='text: our future financial and operational performance and operational expenditures, including our expectations regarding our revenue, cost of revenue, gross margin and operating expenses; maintaining and expanding our customer base and increasing our average subscription revenue per customer; the impact of competition in our industry and innovation by our competitors including as a result of new or better use of evolving artificial intelligence technologies; our anticipated growth and expectations regarding our ability to manage our future growth; our expectations regarding the potential impact of geo-political conflicts, inflationary pressures, foreign currency movement, macroeconomic stability, and catastrophic events, such as the covid-19 pandemic, on our business, the broader economy, our workforce and operations, the markets in which we and our partners and customers operate, and our ability to forecast future financial performance; our anticipated areas of investments, including'),\n",
       " Document(metadata={'source': '../data/edgar_filings2.csv', 'row': 2, 'company': 'HUBS', 'date': '2024-02-14', 'section': 'business'}, page_content='our workforce and operations, the markets in which we and our partners and customers operate, and our ability to forecast future financial performance; our anticipated areas of investments, including sales and marketing, research and development including with respect to artificial intelligence and machine learning, customer service and support, data center infrastructure and service capabilities, and expectations relating to such investments; our predictions about industry and market trends; our ability to anticipate and address the evolution of technology and the technological needs of our customers, to roll-out upgrades to our existing software platform and to develop new and enhanced applications to meet the needs of our customers including with respect to artificial intelligence and machine learning; our ability to maintain our brand and inbound marketing, selling and servicing thought leadership position; the impact of our corporate culture and our ability to attract, hire and'),\n",
       " Document(metadata={'source': '../data/edgar_filings2.csv', 'row': 2, 'company': 'HUBS', 'date': '2024-02-14', 'section': 'business'}, page_content='machine learning; our ability to maintain our brand and inbound marketing, selling and servicing thought leadership position; the impact of our corporate culture and our ability to attract, hire and retain necessary qualified employees to expand our operations; the anticipated effect on our business of litigation to which we are or may become a party; our ability to successfully acquire and integrate companies and assets; our plans regarding declaring or paying cash dividends in the foreseeable future; and our ability to stay abreast of new or modified laws and regulations that currently apply or become applicable to our business both in the united states and internationally. we caution you that the foregoing list may not contain all of the forward-looking statements made in this annual report on form 10-k. you should not rely upon forward-looking statements as predictions of future events. we have based the forward-looking statements contained in this annual report on form 10-k'),\n",
       " Document(metadata={'source': '../data/edgar_filings2.csv', 'row': 2, 'company': 'HUBS', 'date': '2024-02-14', 'section': 'business'}, page_content='report on form 10-k. you should not rely upon forward-looking statements as predictions of future events. we have based the forward-looking statements contained in this annual report on form 10-k primarily on our current expectations and projections about future events and trends that we believe may affect our business, financial condition, results of operations and prospects. the outcome of the events described in these forward-looking statements is subject to risks, uncertainties and other factors described in “risk factors” and elsewhere in this annual report on form 10-k. moreover, we operate in a very competitive and rapidly changing environment. new risks and uncertainties emerge from time to time, and it is not possible for us to predict all risks and uncertainties that could have an impact on the forward-looking statements contained in this annual report on form 10-k. the results, events and circumstances reflected in the forward-looking statements may not be achieved or'),\n",
       " Document(metadata={'source': '../data/edgar_filings2.csv', 'row': 2, 'company': 'HUBS', 'date': '2024-02-14', 'section': 'business'}, page_content='have an impact on the forward-looking statements contained in this annual report on form 10-k. the results, events and circumstances reflected in the forward-looking statements may not be achieved or occur, and actual results, events or circumstances could differ materially from those described in the forward-looking statements. the forward-looking statements made in this annual report on form 10-k relate only to events as of the date on which the statements are made. we undertake no obligation to update any forward-looking statements made in this annual report on form 10-k to reflect events or circumstances after the date of this annual report on form 10-k or to reflect new information or the occurrence of unanticipated events, except as required by law. we may not actually achieve the plans, intentions or expectations disclosed in our forward-looking statements and you should not place undue reliance on our forward-looking statements. our forward-looking statements do not reflect'),\n",
       " Document(metadata={'source': '../data/edgar_filings2.csv', 'row': 2, 'company': 'HUBS', 'date': '2024-02-14', 'section': 'business'}, page_content='plans, intentions or expectations disclosed in our forward-looking statements and you should not place undue reliance on our forward-looking statements. our forward-looking statements do not reflect the potential impact of any future acquisitions, mergers, dispositions, joint ventures, or investments we may make. in this annual report on form 10-k, the terms “hubspot,” “we,” “us,” and “our” refer to hubspot, inc. and its subsidiaries, unless the context indicates otherwise.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    azure_endpoint='https://aiall9596864698.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15',\n",
    "    api_key='6d440529fad24ffc8aef6d5f9ef52593',\n",
    "    openai_api_version=\"2024-07-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 01:22:19,021 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"test_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"../data/chroma_langchain_db_test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 00:19:35,026 - INFO - HTTP Request: POST https://aiall9596864698.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-07-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    [text],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 01:23:46,885 - INFO - HTTP Request: POST https://aiall9596864698.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-07-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['01187911-03e8-4f18-85f6-96b55b6f61ca',\n",
       " 'a8ce9964-a8d1-4941-b6f4-b90cf9afdd39',\n",
       " '38e339a4-58f6-4479-9d48-e3358f66e6be',\n",
       " '8d87771f-b249-48f6-9a3d-fb214796cd1c',\n",
       " '5f6d0a14-b1a6-47e9-a6f2-917b5bedaf1f',\n",
       " '3137dca4-efab-4fa4-8774-80226c82b262',\n",
       " 'f514500d-158a-47ff-a584-d1d4c115d802',\n",
       " '2d1326ce-79bf-4572-9f18-1a8c10021609',\n",
       " '729fded3-f747-4cad-ad7f-0a4153bc6c2d',\n",
       " 'd11e5d12-ae35-4395-960c-375ef42c2d21']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "uuids = [str(uuid4()) for _ in range(len(test_texts))]\n",
    "vector_store.add_documents(documents=test_texts, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import langgraph\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool\n",
    "import langchain_core\n",
    "import langchain_openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import Tool, AgentExecutor, create_react_agent, create_tool_calling_agent, initialize_agent, create_openai_tools_agent\n",
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "import orjson\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "from typing import Literal, Union, Sequence, TypedDict, Annotated\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import functools\n",
    "import operator\n",
    "import logging\n",
    "import streamlit as st\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from typing import Optional\n",
    "from langchain.retrievers.self_query.chroma import ChromaTranslator\n",
    "from langchain.chains.query_constructor.ir import Comparator, Comparison, Operation, Operator, StructuredQuery\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"]=\"b229fdc63a3d4b06b4adcc67660474f3\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"]=\"https://openai-541.openai.azure.com/\"\n",
    "os.environ[\"ALPHAVANTAGE_API_KEY\"]=\"5TW2G5K4N9GKMGRD\"\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "#logging.disable(logging.CRITICAL)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "original_stdout = sys.stdout  # Save a reference to the original stdout\n",
    "captured_output = StringIO()  # Create a new StringIO object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 20:05:12.883 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:12.891 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:12.891 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:12.891 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:12.899 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:12.907 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:12.907 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "C:\\Users\\Dave\\AppData\\Local\\Temp\\ipykernel_10252\\1682207930.py:266: LangChainDeprecationWarning: The method `BaseChatOpenAI.bind_functions` was deprecated in langchain-openai 0.2.1 and will be removed in 0.3.0. Use :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind_tools` instead.\n",
      "  | self.agent_manager.llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
      "2024-12-01 20:05:13.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:13.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:13.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:13.435 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:13.435 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:13.443 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:13.443 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-01 20:05:13.451 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "class ChromaRetrieverTool:\n",
    "    def __init__(self, directory, collection, embedding, llm):\n",
    "        self.directory = directory\n",
    "        self.collection = collection\n",
    "        self.embedding = embedding\n",
    "        self.llm = llm\n",
    "\n",
    "    def initiate_retriever(self):\n",
    "        metadata_field_info = [\n",
    "            AttributeInfo(\n",
    "                name=\"date\",\n",
    "                description=\"The date of 10-k filing\",\n",
    "                type=\"string or list[string]\",\n",
    "            ),\n",
    "            AttributeInfo(\n",
    "                name=\"section\",\n",
    "                description=\"The section of the 10-k filing\",\n",
    "                type=\"string or list[string]\",\n",
    "            ),\n",
    "            AttributeInfo(\n",
    "                name=\"company\",\n",
    "                description=\"The name of the company\",\n",
    "                type=\"string or list[string]\",\n",
    "            ),\n",
    "            ]\n",
    "        \n",
    "        document_content_description = \"Chunked content of the 10-k filing\"\n",
    "\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=self.collection,\n",
    "            persist_directory=self.directory,\n",
    "            embedding_function=self.embedding\n",
    "        )\n",
    "\n",
    "        self.retriever=SelfQueryRetriever.from_llm(\n",
    "            self.llm,\n",
    "            self.vector_store,\n",
    "            document_content_description,\n",
    "            metadata_field_info,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        self.retriever_tool=create_retriever_tool(\n",
    "            self.retriever,\n",
    "            '10-k document retriever',\n",
    "            'Query a retriever to get 10-k document information',\n",
    "        )\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State dictionary for the agent.\n",
    "    \n",
    "    Holds the messages and the next state to transition to.\n",
    "    \"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    next: str\n",
    "    #intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "\n",
    "def create_agent(llm: AzureChatOpenAI, tools: list, system_prompt: str):\n",
    "    \"\"\"Creates an agent using the provided LLM and tools.\n",
    "    \n",
    "    Sets up a prompt template and creates an agent executor.\n",
    "    \n",
    "    :param llm: Language model to be used by the agent.\n",
    "    :param tools: List of tools available to the agent.\n",
    "    :param system_prompt: System-level prompt for the agent.\n",
    "    :return: AgentExecutor instance configured with the provided settings.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    \n",
    "    return executor\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    \"\"\"Executes the agent with the given state and returns the result.\n",
    "    \n",
    "    Handles errors and formats the result into a message.\n",
    "    \n",
    "    :param state: Current state of the agent.\n",
    "    :param agent: Agent instance to be executed.\n",
    "    :param name: Name of the agent for identifying the source of the message.\n",
    "    :return: Dictionary containing the result message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = agent.invoke(state)\n",
    "\n",
    "        return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n",
    "    except Exception as e:\n",
    "        print(f\"Error with agent: {e}\")\n",
    "        return {\"messages\": [HumanMessage(content=\"Error occurred\", name=name)]}\n",
    "    \n",
    "def create_llm(\n",
    "    model: str = \"gpt-35-turbo\",\n",
    "    api_version: str = \"2024-07-01-preview\",\n",
    "    temperature: float = 0,\n",
    "    max_tokens: int = 500,\n",
    "    timeout: int = None,\n",
    "    max_retries: int = 2\n",
    ") -> AzureChatOpenAI:\n",
    "    \"\"\"Creates an instance of ChatOpenAI with specified parameters.\n",
    "    \n",
    "    :param model: The model to use (default is \"gpt-4o\").\n",
    "    :param temperature: Controls the randomness of the output (default is 0).\n",
    "    :param max_tokens: Maximum number of tokens in the output (default is 500).\n",
    "    :param timeout: Timeout duration for the request (default is None).\n",
    "    :param max_retries: Number of retry attempts in case of failure (default is 2).\n",
    "    :return: An instance of ChatOpenAI configured with the given parameters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        llm = AzureChatOpenAI(\n",
    "            azure_deployment=model,  # or your deployment\n",
    "            api_version=api_version,  # or your api version\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "        )\n",
    "\n",
    "        return llm\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log error message and re-raise exception for further handling if needed\n",
    "        print(f\"Error with LLM creation: {e}\")\n",
    "        raise\n",
    "\n",
    "class AgentManager:\n",
    "    def __init__(self, llm):\n",
    "        \"\"\"\n",
    "        Initialize the AgentManager with the given language model.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model instance to use for creating agents.\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Initialize tools for different agents\n",
    "        chroma_retriever = ChromaRetrieverTool(directory=\"../data/chroma_langchain_db_test\", collection=\"test_collection\", embedding=embeddings, llm=create_llm())\n",
    "        chroma_retriever.initiate_retriever()\n",
    "        self.tenk_tools = [chroma_retriever.retriever_tool]\n",
    "        \n",
    "        # Create the agents\n",
    "        self.create_agents()\n",
    "        \n",
    "    def create_agents(self):\n",
    "        \"\"\"\n",
    "        Create agents for Snowflake table operations, SQL querying, and documentation searching.\n",
    "        \"\"\"\n",
    "        # Create Snowflake Expert Agent\n",
    "        tenk_agent_system_prompt = (\n",
    "            \"You are an expert in 10-k filings. You can provide information about a company's 10-k filings.\"\n",
    "        )\n",
    "        self.tenk_agent = create_agent(\n",
    "            self.llm, self.tenk_tools, tenk_agent_system_prompt\n",
    "        )\n",
    "        self.tenk_node = functools.partial(agent_node, agent=self.tenk_agent, name=\"Ten-K-Expert\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Create SQL Coder Agent\n",
    "        #sql_agent_tools = [SnowparkQueryTool(snowpark_adapter=self.snowpark_adapter)]\n",
    "        #sql_agent_system_prompt = \"You are a Snowflake SQL expert.\"\n",
    "        #self.sql_agent = create_agent(self.llm, sql_agent_tools, sql_agent_system_prompt)\n",
    "        #self.sql_node = functools.partial(agent_node, agent=self.sql_agent, name=\"SQL-Coder\")\n",
    "        \n",
    "        # Create Documentation Agent\n",
    "        #documentation_agent_system_prompt = \"You are an expert in the company's documentation.\"\n",
    "        #self.documentation_agent = create_agent(self.llm, self.documentation_tools, documentation_agent_system_prompt)\n",
    "        #self.documentation_node = functools.partial(agent_node, agent=self.documentation_agent, name=\"Documentation-Expert\")\n",
    "\n",
    "        \n",
    "    def get_nodes(self):\n",
    "        \"\"\"\n",
    "        Retrieve the nodes for each agent.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary where keys are agent names and values are their corresponding nodes.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Ten-K-Expert\": self.tenk_node,\n",
    "            #\"SQL-Coder\": self.sql_node,\n",
    "            #\"Snowflake-Expert\": self.snowflake_node\n",
    "        }\n",
    "\n",
    "class WorkflowManager:\n",
    "    def __init__(self, agent_manager):\n",
    "        \"\"\"\n",
    "        Initialize the WorkflowManager with the given AgentManager.\n",
    "        \n",
    "        Args:\n",
    "            agent_manager: The instance of AgentManager to manage the workflow.\n",
    "        \"\"\"\n",
    "        self.agent_manager = agent_manager\n",
    "        self.workflow = StateGraph(AgentState)\n",
    "        self.setup_workflow()\n",
    "    \n",
    "    def setup_workflow(self):\n",
    "        \"\"\"\n",
    "        Set up the workflow by adding nodes, configuring the supervisor, defining edges, and compiling the graph.\n",
    "        \"\"\"\n",
    "        # Add nodes for each agent to the workflow\n",
    "        for role, node in self.agent_manager.get_nodes().items():\n",
    "            self.workflow.add_node(role, node)\n",
    "        \n",
    "        # Add the supervisor node to the workflow\n",
    "        self.setup_supervisor()\n",
    "        \n",
    "        # Define the edges in the workflow\n",
    "        self.define_edges()\n",
    "        \n",
    "        # Compile the graph to finalize the workflow setup\n",
    "        self.graph = self.workflow.compile()\n",
    "    \n",
    "    def setup_supervisor(self):\n",
    "        \"\"\"\n",
    "        Set up the supervisor node to manage the conversation between agents.\n",
    "        \"\"\"\n",
    "        members = list(self.agent_manager.get_nodes().keys())\n",
    "        options = [\"FINISH\"] + members\n",
    "        system_prompt = (\n",
    "            \"You are a supervisor tasked with managing a conversation between the \"\n",
    "            \"following workers: {members}. Then respond with the worker to act next. Each worker will perform a task and respond with their results \"\n",
    "            \"and status. Continue this until you have enough information.\"\n",
    "        )\n",
    "        \n",
    "        supervisor_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Given the conversation above, who should act next? \"\n",
    "                    \"Or should we FINISH? Summarize the conversation always relating it to the original question then Select one of: {options}.\",\n",
    "                ),\n",
    "            ]\n",
    "        ).partial(options=str(options), members=\", \".join(members))\n",
    "        \n",
    "        # Define function for routing based on supervisor's decision\n",
    "        function_def = {\n",
    "            \"name\": \"route\",\n",
    "            \"description\": \"Select the next role.\",\n",
    "            \"parameters\": {\n",
    "                \"title\": \"routeSchema\",\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"next\": {\n",
    "                        \"title\": \"Next\",\n",
    "                        \"anyOf\": [{\"enum\": options}],\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"next\"],\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        self.supervisor_chain = (\n",
    "            supervisor_prompt\n",
    "            | self.agent_manager.llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "            | JsonOutputFunctionsParser()\n",
    "        )\n",
    "        # Add the supervisor chain as a node in the workflow\n",
    "        self.workflow.add_node(\"supervisor\", self.supervisor_chain)\n",
    "    \n",
    "    def define_edges(self):\n",
    "        \"\"\"\n",
    "        Define the edges between nodes in the workflow.\n",
    "        \"\"\"\n",
    "        members = list(self.agent_manager.get_nodes().keys())\n",
    "        # Add edges from each member to the supervisor\n",
    "        for member in members:\n",
    "            self.workflow.add_edge(member, \"supervisor\")\n",
    "        \n",
    "        # Map member names to corresponding nodes and define conditional edges\n",
    "        conditional_map = {k: k for k in members}\n",
    "        conditional_map[\"FINISH\"] = END\n",
    "        self.workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "        # Add edge from the start node to the supervisor\n",
    "        self.workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "def my_query(myquery, graph):\n",
    "    \"\"\"\n",
    "    Initiates the workflow graph with the provided query and streams the results.\n",
    "\n",
    "    This function takes a user query, feeds it into the specified workflow graph, and processes the results as they are produced. \n",
    "    It prints each step of the result until the workflow reaches the end.\n",
    "\n",
    "    Args:\n",
    "        myquery (str): The user query to be processed by the workflow graph.\n",
    "        graph (StateGraph): The workflow graph instance that manages the execution and processing of the query.\n",
    "\n",
    "    Usage:\n",
    "        Call this function with a query string and a graph instance to start the workflow and see the intermediate results.\n",
    "\n",
    "    Example:\n",
    "        my_query(\"change password\", graph_instance)\n",
    "    \"\"\"\n",
    "    # Stream the results of the graph execution based on the provided query\n",
    "\n",
    "    final_state=graph.invoke(\n",
    "        {\"messages\": [HumanMessage(content=myquery)]},\n",
    "        {\"recursion_limit\": 100},\n",
    "        #return_intermediate_steps=True,\n",
    "    )\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo\",  # or your deployment\n",
    "    api_version=\"2024-07-01-preview\",  # or your api version\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #st.session_state.messages.append({\"text\": final.content, \"role\": \"assistant\"})\n",
    "\n",
    "    #st.markdown(final.content)\n",
    "    #st.markdown(final_state[\"messages\"][-1].content)\n",
    "\n",
    "    return final_state, final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Main function to execute the workflow.\n",
    "    \n",
    "    This function sets up a Snowflake connection, initializes components such as the SQL adapter \n",
    "    and language model, creates agents and workflow, and then executes a query.\n",
    "    \"\"\"\n",
    "    st.title(\"Ask Questions to your Own Personal Data Assistant:\")\n",
    "    st.write(\"\"\"Ask your questions here.\"\"\")\n",
    "    \n",
    "\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    # Initialize components\n",
    "    llm = create_llm()\n",
    "    \n",
    "    if not llm:\n",
    "        # Log error if LLM instance creation fails\n",
    "        logging.error(\"Failed to create LLM instance.\")\n",
    "        st.error(\"Failed to create LLM instance.\")\n",
    "        return\n",
    "\n",
    "    # Create agents and workflow\n",
    "    try:\n",
    "        # Initialize AgentManager with LLM and SQL adapter\n",
    "        agent_manager = AgentManager(llm)\n",
    "        # Initialize WorkflowManager with the created agents\n",
    "        workflow_manager = WorkflowManager(agent_manager)\n",
    "        # Access the compiled workflow graph\n",
    "        graph = workflow_manager.graph\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log any exceptions that occur during agent or workflow setup\n",
    "        logging.error(f\"An error occurred while setting up agents or workflow: {e}\")\n",
    "        return\n",
    "\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "    \n",
    "    # React to user input\n",
    "    if myquery := st.chat_input(\"What is up?\"):\n",
    "        # Display user message in chat message container\n",
    "        st.chat_message(\"user\").markdown(myquery)\n",
    "        # Add user message to chat history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": myquery})\n",
    "    \n",
    "        try:\n",
    "            # Execute the query and process results\n",
    "            final_state, final=my_query(myquery, graph)\n",
    "        except Exception as e:\n",
    "            # Log any exceptions that occur during query execution\n",
    "            logging.error(f\"An error occurred while executing the query: {e}\")\n",
    "        \n",
    "    \n",
    "        response = f\"Bot: {final.content}\"\n",
    "        # Display assistant response in chat message container\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(response)\n",
    "        \n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return final_state, final\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stdout = captured_output\n",
    "\n",
    "\n",
    "    final_state=main()\n",
    "    sys.stdout = original_stdout\n",
    "    output = captured_output.getvalue()\n",
    "\n",
    "    with st.expander(\"Verbose Output\", expanded=False):\n",
    "        if output:\n",
    "            st.text(output)  # or st.write(output) for more flexibility\n",
    "    with st.expander(\"Agent Conversation\", expanded=False):\n",
    "        if final_state:\n",
    "            for i in final_state[0][\"messages\"]:\n",
    "                st.text(i.pretty_repr())  # Display each message as plain text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo\",  # or your deployment\n",
    "    api_version=\"2024-07-01-preview\",  # or your api version\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
